{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkbahk/QuantumComputing/blob/main/QML_mnist_example_pennylane.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90ZXxqEfhw_X"
      },
      "source": [
        "This cell imports all required libraries. We use pytorch to define and train neural networks as well as provide the classical layers. We use torchvision to load the MNIST digits dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install pennylane\n",
        "!python3 -m pip install pennylane-lightning[gpu]\n",
        "!python3 -m pip install pennylane-qulacs\n",
        "!python3 -m pip install qulacs\n",
        "!python3 -m pip install torch\n",
        "!python3 -m pip install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxf3d8QKiDVa",
        "outputId": "5c580c8a-165a-47cb-8ba7-8fd9bf7eaf90"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.33.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.2.1)\n",
            "Collecting rustworkx (from pennylane)\n",
            "  Downloading rustworkx-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Collecting semantic-version>=2.7 (from pennylane)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting autoray>=0.6.1 (from pennylane)\n",
            "  Downloading autoray-0.6.7-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.2)\n",
            "Collecting pennylane-lightning>=0.33 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.5.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2023.11.17)\n",
            "Installing collected packages: semantic-version, rustworkx, autoray, pennylane-lightning, pennylane\n",
            "Successfully installed autoray-0.6.7 pennylane-0.33.1 pennylane-lightning-0.33.1 rustworkx-0.13.2 semantic-version-2.10.0\n",
            "Requirement already satisfied: pennylane-lightning[gpu] in /usr/local/lib/python3.10/dist-packages (0.33.1)\n",
            "Requirement already satisfied: pennylane>=0.32 in /usr/local/lib/python3.10/dist-packages (from pennylane-lightning[gpu]) (0.33.1)\n",
            "Collecting pennylane-lightning-gpu (from pennylane-lightning[gpu])\n",
            "  Downloading PennyLane_Lightning_GPU-0.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.32->pennylane-lightning[gpu]) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.32->pennylane-lightning[gpu]) (1.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.32->pennylane-lightning[gpu]) (3.2.1)\n",
            "Requirement already satisfied: rustworkx in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.32->pennylane-lightning[gpu]) (0.13.2)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.32->pennylane-lightning[gpu]) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.32->pennylane-lightning[gpu]) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.32->pennylane-lightning[gpu]) (1.4.4)\n",
            "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.32->pennylane-lightning[gpu]) (2.10.0)\n",
            "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.32->pennylane-lightning[gpu]) (0.6.7)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.32->pennylane-lightning[gpu]) (5.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.32->pennylane-lightning[gpu]) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.32->pennylane-lightning[gpu]) (4.5.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane>=0.32->pennylane-lightning[gpu]) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane>=0.32->pennylane-lightning[gpu]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane>=0.32->pennylane-lightning[gpu]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane>=0.32->pennylane-lightning[gpu]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane>=0.32->pennylane-lightning[gpu]) (2023.11.17)\n",
            "Installing collected packages: pennylane-lightning-gpu\n",
            "Successfully installed pennylane-lightning-gpu-0.33.1\n",
            "Collecting pennylane-qulacs\n",
            "  Downloading pennylane_qulacs-0.32.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pennylane>=0.15 in /usr/local/lib/python3.10/dist-packages (from pennylane-qulacs) (0.33.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pennylane-qulacs) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane-qulacs) (1.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.15->pennylane-qulacs) (3.2.1)\n",
            "Requirement already satisfied: rustworkx in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.15->pennylane-qulacs) (0.13.2)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.15->pennylane-qulacs) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.15->pennylane-qulacs) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.15->pennylane-qulacs) (1.4.4)\n",
            "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.15->pennylane-qulacs) (2.10.0)\n",
            "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.15->pennylane-qulacs) (0.6.7)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.15->pennylane-qulacs) (5.3.2)\n",
            "Requirement already satisfied: pennylane-lightning>=0.33 in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.15->pennylane-qulacs) (0.33.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.15->pennylane-qulacs) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.15->pennylane-qulacs) (4.5.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane>=0.15->pennylane-qulacs) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane>=0.15->pennylane-qulacs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane>=0.15->pennylane-qulacs) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane>=0.15->pennylane-qulacs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane>=0.15->pennylane-qulacs) (2023.11.17)\n",
            "Installing collected packages: pennylane-qulacs\n",
            "Successfully installed pennylane-qulacs-0.32.0\n",
            "Collecting qulacs\n",
            "  Downloading qulacs-0.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (870 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m870.3/870.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from qulacs) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from qulacs) (1.11.4)\n",
            "Installing collected packages: qulacs\n",
            "Successfully installed qulacs-0.6.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dw6JWwHDhw_Z"
      },
      "outputs": [],
      "source": [
        "# pytorch dataset loading, model definition, and model training code\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# pennylane imports for defining the quantum part of the model\n",
        "import pennylane as qml\n",
        "import pennylane.numpy as np\n",
        "\n",
        "\n",
        "# for timing the training process\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQADu368hw_a"
      },
      "source": [
        "Here we handle loading the MNIST digits and optionally select a reduced set of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5emmRvZhw_a",
        "outputId": "9149e4c2-20db-4cc4-ba15-2fcddd52c6db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/pytorch_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 214922645.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/pytorch_data/MNIST/raw/train-images-idx3-ubyte.gz to /root/pytorch_data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/pytorch_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 7394438.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/pytorch_data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/pytorch_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/pytorch_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 226260923.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/pytorch_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/pytorch_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/pytorch_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 18513633.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/pytorch_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/pytorch_data/MNIST/raw\n",
            "\n",
            "Train data:\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: /root/pytorch_data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "\n",
            "Test data:\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: /root/pytorch_data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ],
      "source": [
        "# load the MNIST training and testing datasets\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"~/pytorch_data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"~/pytorch_data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "# allow for restricting classes to specific digits\n",
        "from functools import reduce\n",
        "def restrict_classes(dataset, classes):\n",
        "    classes_membership_mask = reduce(lambda a,b: a|b,\n",
        "                                     [dataset.targets == i for i in classes],\n",
        "                                     torch.zeros(dataset.targets.size(), dtype=int))\n",
        "    idx = torch.where(classes_membership_mask)\n",
        "    dataset.data = dataset.data[idx]\n",
        "    dataset.targets = dataset.targets[idx]\n",
        "    return dataset\n",
        "###def\n",
        "\n",
        "# NOTE: you can change this list to select which digit classes are\n",
        "# used from the datasets.\n",
        "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "print(\"Train data:\")\n",
        "print(restrict_classes(training_data, classes))\n",
        "print()\n",
        "print(\"Test data:\")\n",
        "print(restrict_classes(test_data, classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Ztqsb7hw_b"
      },
      "source": [
        "This cell defines the hybrid model. We first define the quantum part, then the hybrid model class that makes use of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kmIqkZ5phw_b"
      },
      "outputs": [],
      "source": [
        "# NOTE: here, we configure the hyperparameters of the quantum layer.\n",
        "# You must install the lightning.qubit device for cpu runs or\n",
        "# lightning.gpu device for gpu runs. You can also set use_lightning=False\n",
        "# to use the slow default.qubit backend.\n",
        "nqubits = 4\n",
        "nlayers = 2\n",
        "use_lightning = False\n",
        "use_gpu = False\n",
        "\n",
        "# default to cpu pytorch ops\n",
        "torch_device = \"cpu\"\n",
        "if use_lightning:\n",
        "    if use_gpu:\n",
        "        qml_device = qml.device('lightning.gpu', wires=nqubits)\n",
        "        # override to use gpu in pytorch\n",
        "        torch_device = \"cuda\"\n",
        "    else:\n",
        "        qml_device = qml.device('lightning.qubit', wires=nqubits)\n",
        "    ###if\n",
        "else:\n",
        "    if use_gpu:\n",
        "        raise RuntimeError(\"Cannot use gpu without also using lightning simulator.\")\n",
        "    ###if\n",
        "    qml_device = qml.device('qulacs.simulator', wires=nqubits)\n",
        "###if\n",
        "\n",
        "# Here we define the quantum part of the model\n",
        "@qml.qnode(qml_device, interface=\"torch\")\n",
        "def qnn_layer(inputs, weights):\n",
        "    # encode inputs from previous layer\n",
        "    # as rotations.\n",
        "    for i in range(nqubits):\n",
        "        qml.RX(inputs[i], wires=i)\n",
        "    ###for\n",
        "\n",
        "    # place gates using the trainable weights\n",
        "    # as parameters.\n",
        "    for layer_index in range(nlayers):\n",
        "        # place the trainable rotations\n",
        "        for i in range(nqubits):\n",
        "            qml.RY(weights[i + layer_index * nqubits], wires=i)\n",
        "        ###for\n",
        "\n",
        "        # place the entangling gates\n",
        "        for i in range(nqubits):\n",
        "            j = (i + 1) % nqubits\n",
        "            qml.CNOT(wires=(i, j))\n",
        "        ###for\n",
        "    ###for\n",
        "\n",
        "    # now, return the pauli Z expectation values\n",
        "    # on each qubit.\n",
        "    return tuple(qml.expval(qml.PauliZ(i)) for i in range(nqubits))\n",
        "###def\n",
        "\n",
        "class HybridClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # to convert image arrays to vectors\n",
        "        self.flatten = nn.Flatten()\n",
        "        # to reduce number of features for input to the qnn\n",
        "        self.reduction_layer = nn.Linear(28*28, nqubits)\n",
        "        # to map the qnn outputs to a class (some values are unused if\n",
        "        # not using all 10 classes)\n",
        "        self.output_layer = nn.Linear(nqubits, 10)\n",
        "\n",
        "        # This is the randomly initialised weights tensor for the quantum layer.\n",
        "        self.qnn_weights = torch.rand(nlayers * nqubits) * np.pi\n",
        "    ###def\n",
        "\n",
        "    def forward(self, x):\n",
        "        # transform image array to a vector\n",
        "        x = self.flatten(x)\n",
        "        # scale pixel values to [0, 1] range\n",
        "        x = x / 255.0\n",
        "        # apply classical dimensionality reduction layer\n",
        "        x = self.reduction_layer(x)\n",
        "        # apply pi*tanh activation to put data into the range from -pi\n",
        "        # to pi\n",
        "        x = torch.tanh(x) * torch.pi\n",
        "        # apply the qnn layer to the input and weights.\n",
        "        # older versions of pennylane do not support batches,\n",
        "        # so we iterate through the batch and apply the qnn manually.\n",
        "        batch_size = x.size(0)\n",
        "        out = torch.empty((batch_size, nqubits), dtype=torch.float)\n",
        "        for batch_index in range(batch_size):\n",
        "            expval_tensors = qnn_layer(x[batch_index], self.qnn_weights)\n",
        "            expval_floats = [t.item() for t in expval_tensors]\n",
        "            out[batch_index] = torch.tensor(expval_floats)\n",
        "        ###for\n",
        "\n",
        "        x = out\n",
        "        # apply output layer to combine qnn outputs to 10 numbers\n",
        "        x = self.output_layer(x)\n",
        "        # just return outputs since we will use cross entropy loss in\n",
        "        # training\n",
        "        return x\n",
        "    ###def\n",
        "###class\n",
        "\n",
        "model = HybridClassifier().to(torch_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCM1EPqihw_c"
      },
      "source": [
        "This cell defines function to run single epochs / passes of training and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Adfwq-Gmhw_c"
      },
      "outputs": [],
      "source": [
        "# NOTE: the below code is all used to train the defined model\n",
        "# and will be run when this file is loaded.\n",
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "epochs = 30\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(torch_device)\n",
        "        y = y.to(torch_device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch+1) * len(X)\n",
        "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}\")\n",
        "        ###if\n",
        "    ###for\n",
        "###def\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss = 0\n",
        "\n",
        "    pred = None\n",
        "    X = None\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(torch_device)\n",
        "            y = y.to(torch_device)\n",
        "            pred = model(X)\n",
        "            predicted_label = torch.argmax(pred, dim=1)\n",
        "            correct += torch.count_nonzero(predicted_label == y)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "        ###for\n",
        "    ###with\n",
        "\n",
        "    print(f\"Accuracy: {correct / (len(dataloader) * batch_size) * 100}\")\n",
        "    test_loss /= num_batches\n",
        "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n",
        "###def"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnDASgnlhw_c"
      },
      "source": [
        "This cell will start the training process when executed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xb96ibkhw_d",
        "outputId": "b651bf80-7e70-4199-f6ac-a51b8a420c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training\n",
            "--------------------------\n",
            "Accuracy: 8.905750274658203\n",
            "Avg loss: 2.365273 \n",
            "\n",
            "Epoch 1\n",
            "--------------------------\n",
            "loss: 2.426853 [   32/60000\n",
            "loss: 2.358748 [ 3232/60000\n",
            "loss: 2.291695 [ 6432/60000\n",
            "loss: 2.334761 [ 9632/60000\n",
            "loss: 2.347747 [12832/60000\n",
            "loss: 2.322639 [16032/60000\n",
            "loss: 2.297662 [19232/60000\n",
            "loss: 2.303993 [22432/60000\n",
            "loss: 2.308602 [25632/60000\n",
            "loss: 2.288624 [28832/60000\n",
            "loss: 2.310496 [32032/60000\n",
            "loss: 2.317599 [35232/60000\n",
            "loss: 2.305528 [38432/60000\n",
            "loss: 2.302873 [41632/60000\n",
            "loss: 2.315138 [44832/60000\n",
            "loss: 2.304488 [48032/60000\n",
            "loss: 2.304734 [51232/60000\n",
            "loss: 2.303722 [54432/60000\n",
            "loss: 2.291229 [57632/60000\n",
            "Accuracy: 11.331869125366211\n",
            "Avg loss: 2.301305 \n",
            "\n",
            "Epoch 2\n",
            "--------------------------\n",
            "loss: 2.293177 [   32/60000\n",
            "loss: 2.296879 [ 3232/60000\n",
            "loss: 2.306974 [ 6432/60000\n",
            "loss: 2.286161 [ 9632/60000\n",
            "loss: 2.307736 [12832/60000\n",
            "loss: 2.306955 [16032/60000\n",
            "loss: 2.303900 [19232/60000\n",
            "loss: 2.297210 [22432/60000\n",
            "loss: 2.304444 [25632/60000\n",
            "loss: 2.281720 [28832/60000\n",
            "loss: 2.312078 [32032/60000\n",
            "loss: 2.316406 [35232/60000\n",
            "loss: 2.302246 [38432/60000\n",
            "loss: 2.306665 [41632/60000\n",
            "loss: 2.317333 [44832/60000\n",
            "loss: 2.304206 [48032/60000\n",
            "loss: 2.305039 [51232/60000\n",
            "loss: 2.303769 [54432/60000\n",
            "loss: 2.290278 [57632/60000\n",
            "Accuracy: 11.331869125366211\n",
            "Avg loss: 2.301288 \n",
            "\n",
            "Epoch 3\n",
            "--------------------------\n",
            "loss: 2.292891 [   32/60000\n",
            "loss: 2.296815 [ 3232/60000\n",
            "loss: 2.307222 [ 6432/60000\n",
            "loss: 2.286013 [ 9632/60000\n",
            "loss: 2.307694 [12832/60000\n",
            "loss: 2.306928 [16032/60000\n",
            "loss: 2.303931 [19232/60000\n",
            "loss: 2.297192 [22432/60000\n",
            "loss: 2.304428 [25632/60000\n",
            "loss: 2.281716 [28832/60000\n",
            "loss: 2.312085 [32032/60000\n",
            "loss: 2.316411 [35232/60000\n",
            "loss: 2.302230 [38432/60000\n",
            "loss: 2.306679 [41632/60000\n",
            "loss: 2.317351 [44832/60000\n",
            "loss: 2.304197 [48032/60000\n",
            "loss: 2.305030 [51232/60000\n",
            "loss: 2.303771 [54432/60000\n",
            "loss: 2.290284 [57632/60000\n",
            "Accuracy: 11.331869125366211\n",
            "Avg loss: 2.301288 \n",
            "\n",
            "Epoch 4\n",
            "--------------------------\n",
            "loss: 2.292898 [   32/60000\n",
            "loss: 2.296810 [ 3232/60000\n",
            "loss: 2.307229 [ 6432/60000\n",
            "loss: 2.286010 [ 9632/60000\n",
            "loss: 2.307701 [12832/60000\n",
            "loss: 2.306931 [16032/60000\n",
            "loss: 2.303933 [19232/60000\n",
            "loss: 2.297191 [22432/60000\n",
            "loss: 2.304428 [25632/60000\n",
            "loss: 2.281718 [28832/60000\n",
            "loss: 2.312087 [32032/60000\n",
            "loss: 2.316412 [35232/60000\n",
            "loss: 2.302230 [38432/60000\n",
            "loss: 2.306680 [41632/60000\n",
            "loss: 2.317352 [44832/60000\n",
            "loss: 2.304196 [48032/60000\n",
            "loss: 2.305029 [51232/60000\n",
            "loss: 2.303771 [54432/60000\n",
            "loss: 2.290285 [57632/60000\n",
            "Accuracy: 11.331869125366211\n",
            "Avg loss: 2.301288 \n",
            "\n",
            "Epoch 5\n",
            "--------------------------\n",
            "loss: 2.292899 [   32/60000\n",
            "loss: 2.296810 [ 3232/60000\n",
            "loss: 2.307230 [ 6432/60000\n",
            "loss: 2.286009 [ 9632/60000\n",
            "loss: 2.307702 [12832/60000\n",
            "loss: 2.306933 [16032/60000\n",
            "loss: 2.303933 [19232/60000\n",
            "loss: 2.297191 [22432/60000\n",
            "loss: 2.304428 [25632/60000\n",
            "loss: 2.281718 [28832/60000\n",
            "loss: 2.312087 [32032/60000\n",
            "loss: 2.316412 [35232/60000\n",
            "loss: 2.302229 [38432/60000\n",
            "loss: 2.306680 [41632/60000\n",
            "loss: 2.317353 [44832/60000\n",
            "loss: 2.304196 [48032/60000\n",
            "loss: 2.305029 [51232/60000\n",
            "loss: 2.303771 [54432/60000\n",
            "loss: 2.290286 [57632/60000\n",
            "Accuracy: 11.331869125366211\n",
            "Avg loss: 2.301288 \n",
            "\n",
            "Epoch 6\n",
            "--------------------------\n",
            "loss: 2.292899 [   32/60000\n",
            "loss: 2.296809 [ 3232/60000\n",
            "loss: 2.307229 [ 6432/60000\n",
            "loss: 2.286010 [ 9632/60000\n",
            "loss: 2.307702 [12832/60000\n",
            "loss: 2.306933 [16032/60000\n",
            "loss: 2.303933 [19232/60000\n",
            "loss: 2.297191 [22432/60000\n",
            "loss: 2.304428 [25632/60000\n",
            "loss: 2.281717 [28832/60000\n",
            "loss: 2.312087 [32032/60000\n",
            "loss: 2.316412 [35232/60000\n",
            "loss: 2.302230 [38432/60000\n",
            "loss: 2.306680 [41632/60000\n",
            "loss: 2.317353 [44832/60000\n",
            "loss: 2.304196 [48032/60000\n",
            "loss: 2.305029 [51232/60000\n",
            "loss: 2.303771 [54432/60000\n",
            "loss: 2.290286 [57632/60000\n",
            "Accuracy: 11.331869125366211\n",
            "Avg loss: 2.301288 \n",
            "\n",
            "Epoch 7\n",
            "--------------------------\n",
            "loss: 2.292899 [   32/60000\n",
            "loss: 2.296810 [ 3232/60000\n",
            "loss: 2.307228 [ 6432/60000\n",
            "loss: 2.286009 [ 9632/60000\n",
            "loss: 2.307702 [12832/60000\n",
            "loss: 2.306933 [16032/60000\n",
            "loss: 2.303933 [19232/60000\n",
            "loss: 2.297191 [22432/60000\n",
            "loss: 2.304428 [25632/60000\n",
            "loss: 2.281717 [28832/60000\n",
            "loss: 2.312086 [32032/60000\n",
            "loss: 2.316412 [35232/60000\n",
            "loss: 2.302230 [38432/60000\n",
            "loss: 2.306679 [41632/60000\n",
            "loss: 2.317353 [44832/60000\n",
            "loss: 2.304196 [48032/60000\n",
            "loss: 2.305029 [51232/60000\n",
            "loss: 2.303771 [54432/60000\n",
            "loss: 2.290285 [57632/60000\n",
            "Accuracy: 11.331869125366211\n",
            "Avg loss: 2.301288 \n",
            "\n",
            "Epoch 8\n",
            "--------------------------\n",
            "loss: 2.292899 [   32/60000\n",
            "loss: 2.298725 [ 3232/60000\n",
            "loss: 2.302603 [ 6432/60000\n",
            "loss: 2.288797 [ 9632/60000\n",
            "loss: 2.303079 [12832/60000\n",
            "loss: 2.303731 [16032/60000\n",
            "loss: 2.303109 [19232/60000\n",
            "loss: 2.298709 [22432/60000\n",
            "loss: 2.305894 [25632/60000\n",
            "loss: 2.287386 [28832/60000\n",
            "loss: 2.309247 [32032/60000\n",
            "loss: 2.314082 [35232/60000\n",
            "loss: 2.304222 [38432/60000\n",
            "loss: 2.303529 [41632/60000\n",
            "loss: 2.317912 [44832/60000\n",
            "loss: 2.304462 [48032/60000\n",
            "loss: 2.304145 [51232/60000\n",
            "loss: 2.303179 [54432/60000\n",
            "loss: 2.288902 [57632/60000\n",
            "Accuracy: 11.331869125366211\n",
            "Avg loss: 2.301130 \n",
            "\n",
            "Epoch 9\n",
            "--------------------------\n",
            "loss: 2.291360 [   32/60000\n",
            "loss: 2.300112 [ 3232/60000\n",
            "loss: 2.302864 [ 6432/60000\n",
            "loss: 2.287269 [ 9632/60000\n",
            "loss: 2.303241 [12832/60000\n",
            "loss: 2.304389 [16032/60000\n",
            "loss: 2.303817 [19232/60000\n",
            "loss: 2.298599 [22432/60000\n",
            "loss: 2.306112 [25632/60000\n",
            "loss: 2.285155 [28832/60000\n",
            "loss: 2.309860 [32032/60000\n",
            "loss: 2.314163 [35232/60000\n",
            "loss: 2.303787 [38432/60000\n",
            "loss: 2.304220 [41632/60000\n",
            "loss: 2.317816 [44832/60000\n",
            "loss: 2.305385 [48032/60000\n",
            "loss: 2.305542 [51232/60000\n",
            "loss: 2.303272 [54432/60000\n",
            "loss: 2.289241 [57632/60000\n",
            "Accuracy: 11.331869125366211\n",
            "Avg loss: 2.301072 \n",
            "\n",
            "Epoch 10\n",
            "--------------------------\n",
            "loss: 2.290566 [   32/60000\n",
            "loss: 2.300889 [ 3232/60000\n",
            "loss: 2.303062 [ 6432/60000\n",
            "loss: 2.286468 [ 9632/60000\n",
            "loss: 2.303374 [12832/60000\n",
            "loss: 2.304768 [16032/60000\n",
            "loss: 2.304220 [19232/60000\n",
            "loss: 2.298559 [22432/60000\n",
            "loss: 2.306249 [25632/60000\n",
            "loss: 2.283978 [28832/60000\n",
            "loss: 2.310206 [32032/60000\n",
            "loss: 2.314211 [35232/60000\n",
            "loss: 2.303577 [38432/60000\n",
            "loss: 2.304605 [41632/60000\n",
            "loss: 2.317778 [44832/60000\n",
            "loss: 2.305912 [48032/60000\n",
            "loss: 2.306305 [51232/60000\n",
            "loss: 2.303345 [54432/60000\n",
            "loss: 2.289448 [57632/60000\n",
            "Accuracy: 11.331869125366211\n",
            "Avg loss: 2.301049 \n",
            "\n",
            "Epoch 11\n",
            "--------------------------\n",
            "loss: 2.290150 [   32/60000\n",
            "loss: 2.301318 [ 3232/60000\n",
            "loss: 2.303193 [ 6432/60000\n",
            "loss: 2.286041 [ 9632/60000\n",
            "loss: 2.303465 [12832/60000\n",
            "loss: 2.304978 [16032/60000\n",
            "loss: 2.304443 [19232/60000\n",
            "loss: 2.298544 [22432/60000\n",
            "loss: 2.306329 [25632/60000\n",
            "loss: 2.283355 [28832/60000\n",
            "loss: 2.310396 [32032/60000\n",
            "loss: 2.314235 [35232/60000\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print(f\"Before training\\n--------------------------\")\n",
        "test_loop(test_dataloader, model, loss_fn)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loss = test_loop(test_dataloader, model, loss_fn)\n",
        "    scheduler.step(test_loss)\n",
        "###for\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "print(\"Done!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13+"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}